Modern AI Applications in Video Gaming
This project demonstrates the integration of advanced AI technologies in video games, focusing on enhancing non-player characters (NPCs) to provide more realistic and interactive gaming experiences. It utilizes a combination of natural language processing, real-time conversation capabilities, and emotional responses driven by modern AI models.

Project Overview
The project aims to revolutionize NPC interactions by incorporating the following features:

AI-Driven NPCs: Develops NPCs that leverage OpenAI's GPT-3.5 language model to engage in dynamic, context-aware conversations with players. This allows for more natural and lifelike interactions within the game environment.
Voice and Emotion Recognition: Integrates Meta Oculus's voice recognition technology to enable seamless transitions between speech and text. Additionally, an emotional model is implemented to simulate lifelike behaviors and emotional responses from NPCs.
Game Prototype: A prototype game is developed using the Unity engine, showcasing the AI-powered NPCs within a simulated environment.
Key Features
Hyper-Realistic NPCs: Combines natural language processing with facial expressions and gestures to create NPCs that appear convincingly real.
Deep Conversational Dynamics: Uses advanced dialogue systems to provide meaningful and emotionally rich interactions with players.
Voice-Driven Immersion: Utilizes voice recognition technology to enhance player immersion and create seamless interactions between the player and NPCs.
Emotive Intelligence: Equips NPCs with the ability to recognize and respond to a range of player emotions, fostering stronger connections during gameplay.
Technical Details
The project is built using the Unity engine and integrates several key components:

ChatGPT.cs: Manages the conversational aspects of the NPCs using OpenAI's GPT model, including sentiment analysis and personality configuration based on player interactions​(ChatGPT)​.
OVRLipSyncContext.cs: Interfaces with the Oculus Lip-Sync engine to process and animate NPCs' lip movements based on voice input​(OVRLipSyncContext)​.
TTSWit.cs: Handles text-to-speech functionality, enabling NPCs to speak dynamically generated dialogue in real-time​(TTSWit)​.
System Design
The system design includes:

Unity Integration: Seamless integration of AI components within the Unity framework to facilitate real-time interaction between players and NPCs.
Technical Architecture: A modular approach that allows for scalable and maintainable code, supporting various AI functionalities.
Results and Analysis
The project demonstrates significant improvements in NPC realism and interactivity. Through extensive testing and user feedback, the following outcomes were observed:

Enhanced player engagement and satisfaction due to lifelike NPC interactions.
Successful integration of advanced AI models, showcasing the potential for future developments in gaming AI​(Dissertation)​.
Future Directions
The project opens avenues for future research and development, including:

Exploring advanced deep learning models to further enhance NPC behaviors.
Improving context awareness and emotional recognition to push the boundaries of immersive gaming experiences.

https://youtu.be/XglIWsSAWKk?si=X5ngIiihfgndqVOg


